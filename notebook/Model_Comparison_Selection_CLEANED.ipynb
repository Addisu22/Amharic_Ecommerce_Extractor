{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0efce84",
   "metadata": {
    "id": "e0efce84"
   },
   "source": [
    "# Task 4: Model Comparison & Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2cef0880",
   "metadata": {
    "id": "2cef0880"
   },
   "outputs": [],
   "source": [
    "# %pip install transformers datasets seqeval\n",
    "# %pip install -U transformers datasets seqeval\n",
    "# !pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07865a63",
   "metadata": {
    "id": "07865a63"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "WUCmdVj-9p5M",
   "metadata": {
    "id": "WUCmdVj-9p5M"
   },
   "outputs": [],
   "source": [
    "def read_conll(path):\n",
    "    sentences, tags = [], []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        tokens, labels = [], []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sentences.append(tokens)\n",
    "                    tags.append(labels)\n",
    "                    tokens, labels = [], []\n",
    "            else:\n",
    "                token, label = line.split()[0], line.split()[-1]\n",
    "                tokens.append(token)\n",
    "                labels.append(label)\n",
    "        if tokens:\n",
    "            sentences.append(tokens)\n",
    "            tags.append(labels)\n",
    "    return sentences, tags\n",
    "\n",
    "tokens, ner_tags = read_conll(\"labeled_amharic_data.conll\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fF9k7MPm9wb7",
   "metadata": {
    "id": "fF9k7MPm9wb7"
   },
   "outputs": [],
   "source": [
    "# Unique labels\n",
    "unique_labels = sorted(set(tag for seq in ner_tags for tag in seq))\n",
    "label2id = {l: i for i, l in enumerate(unique_labels)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "# Encode tags\n",
    "encoded_tags = [[label2id[tag] for tag in seq] for seq in ner_tags]\n",
    "dataset = Dataset.from_dict({\"tokens\": tokens, \"ner_tags\": encoded_tags})\n",
    "\n",
    "# Split\n",
    "split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split[\"train\"]\n",
    "val_dataset = split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "Q42Tj8g8-JTM",
   "metadata": {
    "id": "Q42Tj8g8-JTM"
   },
   "outputs": [],
   "source": [
    "def align_labels(examples, tokenizer):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "    all_labels = []\n",
    "    for i, labels in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        aligned = []\n",
    "        prev = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                aligned.append(-100)\n",
    "            elif word_id != prev:\n",
    "                aligned.append(labels[word_id])\n",
    "            else:\n",
    "                aligned.append(labels[word_id])\n",
    "            prev = word_id\n",
    "        all_labels.append(aligned)\n",
    "    tokenized[\"labels\"] = all_labels\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "_CMN5GhE-LbL",
   "metadata": {
    "id": "_CMN5GhE-LbL"
   },
   "outputs": [],
   "source": [
    "seq_metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=2)\n",
    "    true_preds, true_labels = [], []\n",
    "    for pred, label in zip(preds, p.label_ids):\n",
    "        true_preds.append([id2label[p] for (p, l) in zip(pred, label) if l != -100])\n",
    "        true_labels.append([id2label[l] for (p, l) in zip(pred, label) if l != -100])\n",
    "    results = seq_metric.compute(predictions=true_preds, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "XzILG7uf-M8D",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 763,
     "referenced_widgets": [
      "d02139b2513141058fd443173e2d587c",
      "fc98a0482a5246c98c3acce68732b19f",
      "987f714273774512acdb61a2919b4055",
      "75e029ed94ce4906ba7c124e708cc2ea",
      "197d59e1e9664fe48b72a25626895a1e",
      "a525694f40544389b63333a70735dd7e",
      "5a08a4a7a23e44b4ba95834b4d1401c9",
      "5ac1001a4d1e40a7a8d756951be8d819",
      "2f7f4106c8c948959abb6f0c747faed1",
      "0d68481a9de34b74816dd8a39fe928e0",
      "3f955573781b4d768a83e6c443583e76",
      "c3cabc5e7b2742d29207ff1944570b11",
      "b237e9a8c67644879fafc9ef71067343",
      "e69c84a088c34107af1dbaea4038fb2b",
      "61ac0ca90d9b47e5b0496e3677f24064",
      "65c896c270c8441fba24c4a7b9f14c10",
      "9ebcc18739ca4800969ad0b5799f86c3",
      "8c138583521a429ca06bd80740ce95f5",
      "a229171665ca45b9963e2188711f446f",
      "4952c54a94ed43ce80a66ecd30c8312d",
      "a690b52f01644ec798b1153a5d86d7c8",
      "c83d626af437444a8032f5bf9e7446c8",
      "9b1c2284e621442babe0fa526c899133",
      "06dfbbe23d684b23962f0e0823dce33a",
      "a5efd590fd254ba5af19d4c3f1b8c5f5",
      "b1196e157dc64fe1bdeb5dac6f929894",
      "74aa9415b5cf4e8d8db5ed7cc508f84b",
      "04c61c7e6dd041f68061db96d7b91be1",
      "cae45afac9c3435e918984109f7e8b7a",
      "0ee3bdbbc75841ddbde6a4407bb81665",
      "9d8df785d9af4fa78693e9aabf3f9dfd",
      "827532b081a84211b49b67eef016f4a9",
      "0c93783c6f3342fb83d8baa09e7b48b4",
      "6e1efe588eff4022a5e2a5238eb2f6cc",
      "b951564361ca4b279d9605d7993b1581",
      "c229277e975746369ed6ba23b2675572",
      "12ace69c63104bcf8c6b4cb482d7d0f9",
      "7234e8cab1c94080b6be7715a48451c9",
      "8b48e68c44a947c7b77c10ac81d66ded",
      "0ba06db254084627b6aec53e6cc4c0c1",
      "458dc46b84df43d4bb0a20773f3d3a07",
      "c2ebf6efd21d4f30a0d494b394029f80",
      "748f5c239c9649d9a72eaa697767501c",
      "cadc0baf56a34ab388f50c7b8865684f",
      "e6f3e8aff8ba4a9f84c7a75691130c3e",
      "d20618cfd8b2437498cb51c4ca4215db",
      "4314b24607f64e2cb8c4eeae845033bd",
      "4db8d9d20cfe46a391ad62e5faaa7d67",
      "33c92d2188364ba79116bfcd5018b891",
      "3f540c32ac0f4faa9252de803956c6fa",
      "c655994a34ae40ddb3c4e1a7d21b2946",
      "d748a86357034c419171606f06cc4ef4",
      "a8ed7f70a9bc4db7a94559ac701871f2",
      "39bef7e57d2b4dc2a1d9caccfc910395",
      "a5031ef88bea4d98bc8e5e6b688442b4",
      "531a74fa4e75473c81b410104b8609e7",
      "97a443b431d64406a28b0f4ebae3c491",
      "757653d381434f949d915e33e7a35e54",
      "502a05a290dc45858741d62bd1b1e305",
      "49f423796aea439591a1d4112cd32346",
      "c9bac75c07334544a628375c2f890468",
      "778da2b5610c475b90e60bef1b473fa2",
      "444e69a3c07546eaa609a601b8a2ad63",
      "d711ee53ab824cf296d57a83926356f6",
      "176cc8bdb00a44b4a752f90fb9a631b8",
      "b2938c335c504f51b7c6bfc05cc54a05"
     ]
    },
    "id": "XzILG7uf-M8D",
    "outputId": "8ce5543a-3891-40bd-cbe6-22b0c0050f31"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Fine-tuning XLM-Roberta...\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02139b2513141058fd443173e2d587c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/93 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3cabc5e7b2742d29207ff1944570b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 08:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " XLM-Roberta F1-Score: 0.0000\n",
      "\n",
      "ðŸš€ Fine-tuning mBERT...\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1c2284e621442babe0fa526c899133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/93 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1efe588eff4022a5e2a5238eb2f6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 05:31, Epoch 2.83/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 07:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mBERT F1-Score: 0.0000\n",
      "\n",
      "ðŸš€ Fine-tuning DistilBERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f3e8aff8ba4a9f84c7a75691130c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/93 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531a74fa4e75473c81b410104b8609e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 03:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DistilBERT F1-Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "model_names = {\n",
    "    \"XLM-Roberta\": \"xlm-roberta-base\",\n",
    "    \"mBERT\": \"bert-base-multilingual-cased\",\n",
    "    \"DistilBERT\": \"distilbert-base-multilingual-cased\"\n",
    "}\n",
    "\n",
    "best_model = None\n",
    "best_f1 = 0.0\n",
    "\n",
    "for name, model_ckpt in model_names.items():\n",
    "    print(f\"\\nðŸš€ Fine-tuning {name}...\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_ckpt, num_labels=len(unique_labels))\n",
    "\n",
    "    # Preprocess\n",
    "    tokenized_train = train_dataset.map(lambda x: align_labels(x, tokenizer), batched=True)\n",
    "    tokenized_val = val_dataset.map(lambda x: align_labels(x, tokenizer), batched=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./{name.replace(' ', '_')}_ner\",\n",
    "        eval_strategy=\"steps\",  # evaluate every N steps\n",
    "        save_strategy=\"steps\",        # save every N steps\n",
    "        save_steps=50,               # for example\n",
    "        eval_steps=50,               # same number of steps\n",
    "        load_best_model_at_end=True,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=10,\n",
    "        logging_dir=f\"./logs/{name.replace(' ', '_')}\",\n",
    "        metric_for_best_model=\"f1\",\n",
    "        report_to = \"none\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_result = trainer.evaluate()\n",
    "    print(f\" {name} F1-Score: {eval_result['eval_f1']:.4f}\")\n",
    "\n",
    "    if eval_result[\"eval_f1\"] > best_f1:\n",
    "        best_f1 = eval_result[\"eval_f1\"]\n",
    "        best_model = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "gg9wKtIy-OgL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gg9wKtIy-OgL",
    "outputId": "7c5bad2a-daa4-407b-e0dc-619ac7a8aaac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best model: None with F1-score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\" Best model: {best_model} with F1-score: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G6JPWdVhAL40",
   "metadata": {
    "id": "G6JPWdVhAL40"
   },
   "outputs": [],
   "source": [
    "from nbformat import read, write, NO_CONVERT\n",
    "import json\n",
    "\n",
    "# Load notebook\n",
    "with open(\"Fine_Tune_NER_Model.ipynb\", \"r\", encoding=\"utf-8\") as f:\n",
    "    nb = read(f, as_version=NO_CONVERT)\n",
    "\n",
    "# Remove widgets metadata\n",
    "nb.metadata.pop(\"widgets\", None)\n",
    "\n",
    "# Save cleaned notebook\n",
    "with open(\"Fine_Tune_NER_Model_CLEANED.ipynb\", \"w\", encoding=\"utf-8\") as f:\n",
    "    write(nb, f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
